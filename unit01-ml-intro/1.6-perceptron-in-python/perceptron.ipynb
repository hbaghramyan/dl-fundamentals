{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The perceptron cost function\n",
    "\n",
    "With two-class classification we have a training set of $P points ({ (x_p, y_p) }^P_{p=1} )$ - where (y_p )'s take on just two label values from \\( \\{-1, +1\\} \\) - consisting of two classes which we would like to learn how to distinguish between automatically. As we saw in our discussion of logistic regression, in the simplest instance our two classes of data are largely separated by a hyperplane referred to as the **decision boundary** with each class (largely) lying on either side. Logistic regression determines such a decision boundary by fitting a nonlinear logistic regressor to the dataset, with the separating hyperplane falling out naturally where this logistic function pierces the \\( y = 0 \\) plane. Specifically, we saw how the decision boundary was formally given as a hyperplane\n",
    "\n",
    "\\[ w_0 + x^T w = 0 \\] (1)\n",
    "\n",
    "In separating the two classes we saw how this implied - when hyperplane parameters \\( w_0 \\) and \\( w \\) were chosen well - that for most points in the dataset, those from class +1 lie on the positive side of the hyperplane while those with label −1 lie on the negative side of it, i.e.,\n",
    "\n",
    "\\[ w_0 + x^T_p w > 0 \\] if \\( y_p = +1 \\) (2)\n",
    "\n",
    "\\[ w_0 + x^T_p w < 0 \\] if \\( y_p = −1 \\)\n",
    "\n",
    "While the perceptron approach works within the same framework, its approach to determining the decision boundary is more direct. With the perceptron we aim to directly determine the decision boundary by building a cost function based on these ideal properties, and whose minimum provides optimal weights that reflect these properties as best as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
